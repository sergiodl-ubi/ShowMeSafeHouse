{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e354f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch, torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5fac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./Assets/Models\"\n",
    "model_file = f\"{model_dir}/yv5s-11c.onnx\"\n",
    "names = f\"{model_dir}/11c_names.txt\"\n",
    "camera = \"/dev/video0\"\n",
    "test_image = f\"{model_dir}/test_box.jpeg\"\n",
    "test_video = f\"/home/sergio-de/Movies/TestVideoLabBoxesDisplays.mp4\"\n",
    "\n",
    "net_input_size = 416\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce581da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride: 32 | Inputs: ['images'] | Outputs: ['output']\n",
      "Labels: ['box', 'monitor,', 'refrigerator', 'microwave oven', 'television', 'door', 'bed', 'humidifier', 'printer', 'drawer', 'pc']\n"
     ]
    }
   ],
   "source": [
    "session = ort.InferenceSession(model_file)\n",
    "inputs = [x.name for x in session.get_inputs()]\n",
    "outputs = [x.name for x in session.get_outputs()]\n",
    "\n",
    "meta = session.get_modelmeta().custom_metadata_map\n",
    "names = eval(meta['names'])\n",
    "stride, names = int(meta['stride']), [names[x] for x in names]\n",
    "print(f\"Stride: {stride} | Inputs: {inputs} | Outputs: {outputs}\")\n",
    "print(f\"Labels: {names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860e5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://raw.githubusercontent.com/ultralytics/yolov5/master/utils/augmentations.py\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "def showim(name, im):\n",
    "    cv2.namedWindow(name, cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.startWindowThread()\n",
    "    cv2.setWindowProperty(name, cv2.WND_PROP_TOPMOST, 1)\n",
    "    cv2.imshow(name, im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e690da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 416, 3)\n",
      "(3, 416, 416)\n",
      "(3, 416, 416)\n",
      "(1, 3, 416, 416)\n"
     ]
    }
   ],
   "source": [
    "# Load Image\n",
    "im0 = cv2.imread(test_image)\n",
    "shape = (net_input_size, net_input_size)\n",
    "im = letterbox(im0, new_shape=shape, stride=stride, scaleFill=True, auto=False)[0]\n",
    "print(im.shape)\n",
    "# showim(\"Resized\", im)\n",
    "im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "print(im.shape)\n",
    "im = np.ascontiguousarray(im)  # contiguous attribute\n",
    "print(im.shape)\n",
    "im = im.astype(np.float32)\n",
    "im /= 255\n",
    "im = im[None]\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a083b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10647, 16])\n",
      "tensor([[[5.6869e+00, 5.0823e+00, 1.1613e+01,  ..., 7.7610e-03,\n",
      "          3.7501e-02, 1.0390e-02],\n",
      "         [1.4888e+01, 5.9196e+00, 2.6811e+01,  ..., 1.1135e-02,\n",
      "          3.7146e-02, 1.2119e-02],\n",
      "         [2.0410e+01, 4.3510e+00, 3.3332e+01,  ..., 1.0906e-02,\n",
      "          5.0582e-02, 8.5611e-03],\n",
      "         ...,\n",
      "         [3.3549e+02, 3.8409e+02, 2.7865e+02,  ..., 2.4195e-02,\n",
      "          3.1685e-01, 1.2342e-01],\n",
      "         [3.5968e+02, 3.8287e+02, 2.0404e+02,  ..., 3.7138e-02,\n",
      "          3.1146e-01, 9.4836e-02],\n",
      "         [3.8760e+02, 3.8950e+02, 1.8522e+02,  ..., 6.2329e-02,\n",
      "          2.7714e-01, 6.7724e-02]]])\n"
     ]
    }
   ],
   "source": [
    "y = session.run(outputs, {inputs[0]: im})[0]\n",
    "pred = torch.from_numpy(y).to(device)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2479547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def non_max_suppression(\n",
    "        prediction,\n",
    "        conf_thres=0.25,\n",
    "        iou_thres=0.45,\n",
    "        classes=None,\n",
    "        multi_label=False,\n",
    "        labels=(),\n",
    "        max_det=300,\n",
    "        nm=0,  # number of masks\n",
    "):\n",
    "    \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    device = prediction.device\n",
    "    \n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = prediction.shape[2] - nm - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    mi = 5 + nc  # mask start index\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            lb = labels[xi]\n",
    "            v = torch.zeros((len(lb), nc + nm + 5), device=x.device)\n",
    "            v[:, :4] = lb[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box/Mask\n",
    "        box = xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "        mask = x[:, mi:]  # zero columns if no masks\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:mi] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 5 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "        else:\n",
    "            x = x[x[:, 4].argsort(descending=True)]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if mps:\n",
    "            output[xi] = output[xi].to(device)\n",
    "        if (time.time() - t) > time_limit:\n",
    "            LOGGER.warning(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
