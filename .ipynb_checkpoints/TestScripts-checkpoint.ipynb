{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e354f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import time, json\n",
    "from os.path import basename, dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e5fac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./Assets/Models\"\n",
    "model_file = f\"{model_dir}/yv5s-11c.onnx\"\n",
    "names = f\"{model_dir}/11c_names.txt\"\n",
    "camera = \"/dev/video0\"\n",
    "#test_image = f\"{model_dir}/test_bed.jpeg\"\n",
    "test_image = \"/Users/sergio-de/Movies/testvid/frames/frame-0445.jpeg\"\n",
    "video_frames = \"/Users/sergio-de/Movies/testvid/frames/framepaths.txt\"\n",
    "\n",
    "net_input_size = 416\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce581da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride: 32 | Inputs: ['images'] | Outputs: ['output']\n",
      "Labels: ['box', 'monitor,', 'refrigerator', 'microwave oven', 'television', 'door', 'bed', 'humidifier', 'printer', 'drawer', 'pc']\n"
     ]
    }
   ],
   "source": [
    "session = ort.InferenceSession(model_file)\n",
    "inputs = [x.name for x in session.get_inputs()]\n",
    "outputs = [x.name for x in session.get_outputs()]\n",
    "\n",
    "meta = session.get_modelmeta().custom_metadata_map\n",
    "names = eval(meta['names'])\n",
    "stride, names = int(meta['stride']), [names[x] for x in names]\n",
    "print(f\"Stride: {stride} | Inputs: {inputs} | Outputs: {outputs}\")\n",
    "print(f\"Labels: {names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "860e5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://raw.githubusercontent.com/ultralytics/yolov5/master/utils/augmentations.py\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "def showim(name, im):\n",
    "    cv2.namedWindow(name, cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.startWindowThread()\n",
    "    cv2.setWindowProperty(name, cv2.WND_PROP_TOPMOST, 1)\n",
    "    cv2.imshow(name, im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey()\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def non_max_suppression(\n",
    "        prediction,\n",
    "        conf_thres=0.25,\n",
    "        iou_thres=0.45,\n",
    "        max_det=300,\n",
    "):\n",
    "    \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    device = prediction.device\n",
    "    \n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    mi = 5 + nc  # mask start index\n",
    "    output = [torch.zeros((0, 6), device=prediction.device)] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box/Mask\n",
    "        box = xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "        mask = x[:, mi:]  # zero columns if no masks\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        # best class only\n",
    "        conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
    "        x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "        else:\n",
    "            x = x[x[:, 4].argsort(descending=True)]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            LOGGER.warning(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n",
    "def clip_boxes(boxes, shape):\n",
    "    # Clip boxes (xyxy) to image shape (height, width)\n",
    "    if isinstance(boxes, torch.Tensor):  # faster individually\n",
    "        boxes[:, 0].clamp_(0, shape[1])  # x1\n",
    "        boxes[:, 1].clamp_(0, shape[0])  # y1\n",
    "        boxes[:, 2].clamp_(0, shape[1])  # x2\n",
    "        boxes[:, 3].clamp_(0, shape[0])  # y2\n",
    "    else:  # np.array (faster grouped)\n",
    "        boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "        boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "\n",
    "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n",
    "    # Rescale boxes (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    boxes[:, [0, 2]] -= pad[0]  # x padding\n",
    "    boxes[:, [1, 3]] -= pad[1]  # y padding\n",
    "    boxes[:, :4] /= gain\n",
    "    clip_boxes(boxes, img0_shape)\n",
    "    return boxes\n",
    "\n",
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
    "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
    "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
    "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
    "    return y\n",
    "\n",
    "def prepare_input(im0, net_size, stride):\n",
    "    shape = (net_size, net_size)\n",
    "    im = letterbox(im0, new_shape=shape, stride=stride, scaleFill=True, auto=False)[0] # to (sz, sz, c)\n",
    "    # showim(\"Resized\", im)\n",
    "    print(f\"Original {im.shape}: first 3 pixels {im[0][0]}, {im[0][1]}, {im[0][2]}\")\n",
    "    im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB [to (c, sz, sz)]\n",
    "    print((f\"Transposed {im.shape}: first 3 pixels \"+\n",
    "          f\"[{im[0][0][0]} {im[1][0][0]} {im[2][0][0]}], \"+\n",
    "          f\"[{im[0][0][1]} {im[1][0][1]} {im[2][0][1]}], \"+\n",
    "          f\"[{im[0][0][2]} {im[1][0][2]} {im[2][0][2]}]\"))\n",
    "    im = np.ascontiguousarray(im)  # contiguous attribute\n",
    "    im = im.astype(np.float32)\n",
    "    im /= 255\n",
    "    im = im[None] # to (batch, c, sz, sz)\n",
    "    print((f\"Normalized and batched {im.shape}:\\nfirst 3 pixels\\n\"+\n",
    "          \"[{0:.3f} {1:.3f} {2:.3f}],\\n\".format(im[0][0][0][0],im[0][1][0][0],im[0][2][0][0])+\n",
    "          \"[{0:.3f} {1:.3f} {2:.3f}],\\n\".format(im[0][0][0][1],im[0][1][0][1],im[0][2][0][1])+\n",
    "          \"[{0:.3f} {1:.3f} {2:.3f}]\".format(im[0][0][0][2],im[0][1][0][2],im[0][2][0][2])))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aac7bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Images\n",
    "video = False\n",
    "frames = []\n",
    "\n",
    "if video:\n",
    "    with open(video_frames, \"r\", encoding=\"utf-8\") as fh:\n",
    "        frames = fh.read().splitlines()\n",
    "else:\n",
    "    frames.append(test_image)\n",
    "\n",
    "output_path = dirname(frames[0])\n",
    "output_file = f\"{output_path}/annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e690da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (416, 416, 3): first 3 pixels [179 192 206], [179 192 206], [178 191 205]\n",
      "Transposed (3, 416, 416): first 3 pixels [206 192 179], [206 192 179], [205 191 178]\n",
      "Normalized and batched (1, 3, 416, 416):\n",
      "first 3 pixels\n",
      "[0.808 0.753 0.702],\n",
      "[0.808 0.753 0.702],\n",
      "[0.804 0.749 0.698]\n",
      "Results filtered by NMS: [tensor([[6.5403e+00, 2.2985e+02, 2.4628e+02, 4.1608e+02, 8.1145e-01, 1.0000e+00],\n",
      "        [3.4646e+02, 1.2057e+02, 4.1716e+02, 2.7301e+02, 8.0693e-01, 1.0000e+00],\n",
      "        [1.9999e+02, 2.2368e+02, 2.4290e+02, 2.7959e+02, 3.7397e-01, 0.0000e+00]])]\n",
      "Boxes before scaling: tensor([[  6.5403, 229.8493, 246.2788, 416.0773],\n",
      "        [346.4550, 120.5730, 417.1587, 273.0126],\n",
      "        [199.9869, 223.6785, 242.8982, 279.5877]])\n",
      "Boxes after scaling: tensor([[2.5000e+01, 6.2500e+02, 9.5900e+02, 1.0800e+03, 8.1145e-01, 1.0000e+00],\n",
      "        [1.3490e+03, 2.0000e+02, 1.6200e+03, 7.9300e+02, 8.0693e-01, 1.0000e+00],\n",
      "        [7.7900e+02, 6.0100e+02, 9.4600e+02, 8.1900e+02, 3.7397e-01, 0.0000e+00]])\n",
      "frame-0445.jpeg: 1 box, 2 monitor,s, \n",
      "[{'frame_id': 1, 'filename': '/Users/sergio-de/Movies/testvid/frames/frame-0445.jpeg', 'objects': [{'class_id': 0, 'name': 'box', 'relative_coordinates': {'center_x': 0.5324074029922485, 'center_y': 0.6574074029922485, 'width': 0.10308641940355301, 'height': 0.20185184478759766}, 'confidence': 0.37397438287734985}, {'class_id': 1, 'name': 'monitor,', 'relative_coordinates': {'center_x': 0.9163580536842346, 'center_y': 0.45972222089767456, 'width': 0.16728395223617554, 'height': 0.5490740537643433}, 'confidence': 0.8069338798522949}, {'class_id': 1, 'name': 'monitor,', 'relative_coordinates': {'center_x': 0.3037036955356598, 'center_y': 0.7893518805503845, 'width': 0.5765432119369507, 'height': 0.42129629850387573}, 'confidence': 0.8114544749259949}]}]\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "for idx, frame in enumerate(frames):\n",
    "    idx += 1\n",
    "    im0 = cv2.imread(frame)\n",
    "    im = prepare_input(im0, net_input_size, stride)\n",
    "    \n",
    "    y = session.run(outputs, {inputs[0]: im})[0]\n",
    "    \n",
    "    print(\"## Check for weird output ##\")\n",
    "    for line in y[0]:\n",
    "        if line[0] < 0.0 or line[1] < 0.0 or line[2] < 0.0 or line[3] < 0.0 or line[4] < 0.0 or line[4] > 1.0 or line[5] < 0.0 or line[5] > 12.0\n",
    "            print(f\"[{0:.3f},{1:.3f},{2:.3f},{3:.3f},{4:.3f},{5:.3f}]\".format(*line))\n",
    "        else:\n",
    "            continue\n",
    "    print(\"## ###################### ##\")\n",
    "    \n",
    "    pred = torch.from_numpy(y).to(device)\n",
    "    filtered = non_max_suppression(pred)\n",
    "    print(f\"Results filtered by NMS: {filtered}\")\n",
    "    \n",
    "    det = filtered[0]\n",
    "    gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "    imc = im0  # for save_crop\n",
    "\n",
    "    print(f\"Boxes before scaling: {det[:, :4]}\")\n",
    "    det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "    print(f\"Boxes after scaling: {det}\") # all boxes in format [x1, y1, x2, y2, conf, class]\n",
    "\n",
    "    s = f\"{basename(frame)}: \"\n",
    "    for c in det[:, 5].unique():\n",
    "        n = (det[:, 5] == c).sum()  # detections per class\n",
    "        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "    print(s)\n",
    "\n",
    "    objects = []\n",
    "    for *xyxy, conf, cls in reversed(det):\n",
    "        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "        class_id = int(cls)\n",
    "        line = \"{0},{1},{2},{3},{4},{5:.2f}\".format(class_id, *xywh, conf.item())\n",
    "        #print(line)\n",
    "        obj = {\n",
    "            \"class_id\": class_id,\n",
    "            \"name\": names[class_id],\n",
    "            \"relative_coordinates\": {\n",
    "                \"center_x\": xywh[0],\n",
    "                \"center_y\": xywh[1],\n",
    "                \"width\": xywh[2],\n",
    "                \"height\": xywh[3],\n",
    "            },\n",
    "            \"confidence\": conf.item()\n",
    "        }\n",
    "        #print(obj)\n",
    "        objects.append(obj)\n",
    "    \n",
    "    frame_data = {\n",
    "        \"frame_id\": idx,\n",
    "        \"filename\": frame,\n",
    "        \"objects\": objects,\n",
    "    }\n",
    "    output.append(frame_data)\n",
    "\n",
    "if not video:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "with open(output_file, 'w', encoding=\"utf-8\") as fh:\n",
    "    json.dump(output, fh, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
