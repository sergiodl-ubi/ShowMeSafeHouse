{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e354f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import time, json\n",
    "from os.path import basename, dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5fac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./Assets/Models\"\n",
    "model_file = f\"{model_dir}/yv5s-11c.onnx\"\n",
    "names = f\"{model_dir}/11c_names.txt\"\n",
    "camera = \"/dev/video0\"\n",
    "test_image = f\"{model_dir}/test_bed.jpeg\"\n",
    "video_frames = \"/Users/sergio-de/Movies/testvid/frames/framepaths.txt\"\n",
    "\n",
    "net_input_size = 416\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce581da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ort.InferenceSession(model_file)\n",
    "inputs = [x.name for x in session.get_inputs()]\n",
    "outputs = [x.name for x in session.get_outputs()]\n",
    "\n",
    "meta = session.get_modelmeta().custom_metadata_map\n",
    "names = eval(meta['names'])\n",
    "stride, names = int(meta['stride']), [names[x] for x in names]\n",
    "print(f\"Stride: {stride} | Inputs: {inputs} | Outputs: {outputs}\")\n",
    "print(f\"Labels: {names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://raw.githubusercontent.com/ultralytics/yolov5/master/utils/augmentations.py\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "def showim(name, im):\n",
    "    cv2.namedWindow(name, cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.startWindowThread()\n",
    "    cv2.setWindowProperty(name, cv2.WND_PROP_TOPMOST, 1)\n",
    "    cv2.imshow(name, im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey()\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def non_max_suppression(\n",
    "        prediction,\n",
    "        conf_thres=0.25,\n",
    "        iou_thres=0.45,\n",
    "        max_det=300,\n",
    "):\n",
    "    \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    device = prediction.device\n",
    "    \n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    mi = 5 + nc  # mask start index\n",
    "    output = [torch.zeros((0, 6), device=prediction.device)] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box/Mask\n",
    "        box = xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "        mask = x[:, mi:]  # zero columns if no masks\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        # best class only\n",
    "        conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
    "        x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "        else:\n",
    "            x = x[x[:, 4].argsort(descending=True)]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            LOGGER.warning(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n",
    "def clip_boxes(boxes, shape):\n",
    "    # Clip boxes (xyxy) to image shape (height, width)\n",
    "    if isinstance(boxes, torch.Tensor):  # faster individually\n",
    "        boxes[:, 0].clamp_(0, shape[1])  # x1\n",
    "        boxes[:, 1].clamp_(0, shape[0])  # y1\n",
    "        boxes[:, 2].clamp_(0, shape[1])  # x2\n",
    "        boxes[:, 3].clamp_(0, shape[0])  # y2\n",
    "    else:  # np.array (faster grouped)\n",
    "        boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "        boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "\n",
    "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n",
    "    # Rescale boxes (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    boxes[:, [0, 2]] -= pad[0]  # x padding\n",
    "    boxes[:, [1, 3]] -= pad[1]  # y padding\n",
    "    boxes[:, :4] /= gain\n",
    "    clip_boxes(boxes, img0_shape)\n",
    "    return boxes\n",
    "\n",
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
    "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
    "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
    "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
    "    return y\n",
    "\n",
    "def prepare_input(im0, net_size, stride):\n",
    "    shape = (net_size, net_size)\n",
    "    im = letterbox(im0, new_shape=shape, stride=stride, scaleFill=True, auto=False)[0] # to (sz, sz, c)\n",
    "    # showim(\"Resized\", im)\n",
    "    im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB [to (c, sz, sz)]\n",
    "    # print(im.shape)\n",
    "    im = np.ascontiguousarray(im)  # contiguous attribute\n",
    "    im = im.astype(np.float32)\n",
    "    im /= 255\n",
    "    im = im[None] # to (batch, c, sz, sz)\n",
    "    # print(im.shape)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Images\n",
    "video = True\n",
    "frames = []\n",
    "\n",
    "if video:\n",
    "    with open(video_frames, \"r\", encoding=\"utf-8\") as fh:\n",
    "        frames = fh.read().splitlines()\n",
    "else:\n",
    "    frames.append(test_image)\n",
    "\n",
    "output_path = dirname(frames[0])\n",
    "output_file = f\"{output_path}/annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e690da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for idx, frame in enumerate(frames):\n",
    "    idx += 1\n",
    "    im0 = cv2.imread(frame)\n",
    "    im = prepare_input(im0, net_input_size, stride)\n",
    "    \n",
    "    y = session.run(outputs, {inputs[0]: im})[0]\n",
    "    pred = torch.from_numpy(y).to(device)\n",
    "    filtered = non_max_suppression(pred)\n",
    "    #print(filtered)\n",
    "    \n",
    "    det = filtered[0]\n",
    "    gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "    imc = im0  # for save_crop\n",
    "\n",
    "    #print(det[:, :4])\n",
    "    det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "    #print(det) # all boxes in format [x1, y1, x2, y2, conf, class]\n",
    "\n",
    "    s = f\"{basename(frame)}: \"\n",
    "    for c in det[:, 5].unique():\n",
    "        n = (det[:, 5] == c).sum()  # detections per class\n",
    "        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "    print(s)\n",
    "\n",
    "    objects = []\n",
    "    for *xyxy, conf, cls in reversed(det):\n",
    "        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "        class_id = int(cls)\n",
    "        line = \"{0},{1},{2},{3},{4},{5:.2f}\".format(class_id, *xywh, conf.item())\n",
    "        #print(line)\n",
    "        obj = {\n",
    "            \"class_id\": class_id,\n",
    "            \"name\": names[class_id],\n",
    "            \"relative_coordinates\": {\n",
    "                \"center_x\": xywh[0],\n",
    "                \"center_y\": xywh[1],\n",
    "                \"width\": xywh[2],\n",
    "                \"height\": xywh[3],\n",
    "            },\n",
    "            \"confidence\": conf.item()\n",
    "        }\n",
    "        #print(obj)\n",
    "        objects.append(obj)\n",
    "    \n",
    "    frame_data = {\n",
    "        \"frame_id\": idx,\n",
    "        \"filename\": frame,\n",
    "        \"objects\": objects,\n",
    "    }\n",
    "    output.append(frame_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "with open(output_file, 'w', encoding=\"utf-8\") as fh:\n",
    "    json.dump(output, fh, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
